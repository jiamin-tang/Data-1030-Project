{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# fill in Types since it can be implied from Price\n",
    "a = df.Type.isna()\n",
    "for i in range(len(df.Type)):\n",
    "    if a[i] == True:\n",
    "        if df.Price[i] == \"0\":\n",
    "            df.Type[i] = \"Free\"\n",
    "        else:\n",
    "            df.Type[i] = \"Paid\"\n",
    "            \n",
    "# treat missing values in Content Rating\n",
    "df[\"Content Rating\"].replace(to_replace=np.nan,value = \"missing\", inplace=True)\n",
    "\n",
    "# reserve missing datapoints in Rating as test set\n",
    "df_test = df.iloc[list(np.where(np.isnan(df.Rating))[0])]\n",
    "\n",
    "# drop missing values\n",
    "df.dropna(inplace = True)   #remove missing entries\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# convert the type of Reviews from object to int\n",
    "df['Reviews'] = df['Reviews'].astype(int)\n",
    "\n",
    "# preprocess size\n",
    "df['Size varies'] = df.Size.apply(lambda x: 1 if 'Varies' in x else 0)\n",
    "df.Size = df.Size.replace('Varies with device','0')\n",
    "df.Size = df.Size.str.replace('k','e+3')\n",
    "df.Size = df.Size.str.replace('M','e+6')\n",
    "df.Size = pd.to_numeric(df.Size)\n",
    "df.Size = df.Size/1000000\n",
    "\n",
    "# preprocess price\n",
    "df.Price = df.Price.apply(lambda x: x.strip('$'))\n",
    "df.Price = pd.to_numeric(df.Price)\n",
    "\n",
    "# preprocess genres\n",
    "split_genres1 = df.Genres.apply(lambda x: x.split(';')[0])\n",
    "split_genres2 = df.Genres.apply(lambda x: x.split(';')[-1])\n",
    "df[\"First Genre\"]=split_genres1\n",
    "df[\"Second Genre\"]=split_genres2\n",
    "\n",
    "# preprocess last updated\n",
    "from datetime import datetime,date\n",
    "update_date=pd.to_datetime(df[\"Last Updated\"])\n",
    "df[\"Updated Days\"] = update_date.apply(lambda x:date.today()-datetime.date(x))\n",
    "df[\"Updated Days\"] = df[\"Updated Days\"].astype(int)\n",
    "df[\"Updated Days\"] = df[\"Updated Days\"] / 864e+11\n",
    "\n",
    "# preprocess current ver\n",
    "import re\n",
    "df[\"Current Ver\"]=df[\"Current Ver\"].apply(lambda x: 'Varies with device' if x=='Varies with device'  else  re.findall('^[0-9]\\.[0-9]|[\\d]|\\W*',str(x))[0])\n",
    "df['Current ver varies'] = df['Current Ver'].apply(lambda x: 1 if 'Varies' in x else 0)\n",
    "df[\"Current Ver\"].replace(to_replace=\"Varies with device\",value = \"0\", inplace=True)\n",
    "df[\"Current Ver\"].replace(to_replace=\"\",value = \"0\", inplace=True)\n",
    "df[\"Current Ver\"] = pd.to_numeric(df[\"Current Ver\"])\n",
    "uniq_ver = df[\"Current Ver\"].unique()\n",
    "ord_ver = sorted(uniq_ver)\n",
    "\n",
    "# preprocess android ver\n",
    "df['Android ver varies'] = df['Android Ver'].apply(lambda x: 1 if 'Varies' in x else 0)\n",
    "df[\"Min Ver\"]=df[\"Android Ver\"].apply(lambda x:str(x).split(' and ')[0].split(' - ')[0])\n",
    "df[\"Min Ver\"]=df[\"Min Ver\"].replace('4.4W','4.4')\n",
    "df[\"Max Ver\"]=df[\"Android Ver\"].apply(lambda x:str(x).split(' and ')[-1].split(' - ')[-1])\n",
    "df[\"Min Ver\"].replace(to_replace=\"Varies with device\",value = \"0\", inplace=True)\n",
    "df[\"Max Ver\"].replace(to_replace=\"Varies with device\",value = \"0\", inplace=True)\n",
    "df[\"Min Ver\"]=df[\"Min Ver\"].apply(lambda x: 'Varies with device' if x=='Varies with device'  else  re.findall('^[0-9]\\.[0-9]|[\\d]|\\W*',str(x))[0])\n",
    "df[\"Max Ver\"]=df[\"Max Ver\"].apply(lambda x: 'Varies with device' if x=='Varies with device'  else  ('9.0' if 'up' in x else re.findall('^[0-9]\\.[0-9]|[\\d]|\\W*',str(x)))[0])\n",
    "a1 = df[\"Min Ver\"].unique()\n",
    "ord_min = sorted(a1)\n",
    "a2 = df[\"Max Ver\"].unique()\n",
    "ord_max = sorted(a2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Rating\n",
    "df.drop(columns=['Rating'],inplace=True)\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def ML_pipeline_Ridge(X,y,random_state):\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    test_scores = []\n",
    "    alpha_opt = []\n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "\n",
    "    for train_index, CV_index in kf.split(X_other,y_other):\n",
    "        X_train, X_CV = X_other.iloc[train_index.tolist()], X_other.iloc[CV_index.tolist()]\n",
    "        y_train, y_CV = y_other.iloc[train_index.tolist()], y_other.iloc[CV_index.tolist()]\n",
    "\n",
    "        con_ftrs = ['Reviews','Size','Price','Updated Days']\n",
    "        onehot_ftrs = ['Category','Size varies','Type','Content Rating','First Genre','Second Genre',\\\n",
    "                       'Current Ver','Current ver varies','Min Ver','Max Ver','Android ver varies']\n",
    "        ord_ftrs = ['Installs']\n",
    "        ord_cats = [['1+','5+','10+','50+','100+','500+','1,000+','5,000+','10,000+','50,000+','100,000+','500,000+',\\\n",
    "                     '1,000,000+','5,000,000+','10,000,000+','50,000,000+','100,000,000+','500,000,000+','1,000,000,000+']]\n",
    "\n",
    "        ohe = OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore')\n",
    "        orde = OrdinalEncoder(categories = ord_cats)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_train_con = pd.DataFrame(data=scaler.fit_transform(X_train[con_ftrs]), columns = con_ftrs)\n",
    "        X_train_onehot = pd.DataFrame(data=ohe.fit_transform(X_train[onehot_ftrs]), columns = ohe.get_feature_names())\n",
    "        X_train_ord = pd.DataFrame(data=orde.fit_transform(X_train[ord_ftrs]), columns = ord_ftrs)\n",
    "\n",
    "        X_c_con = pd.DataFrame(data=scaler.transform(X_CV[con_ftrs]), columns = con_ftrs)\n",
    "        X_c_onehot = pd.DataFrame(data=ohe.transform(X_CV[onehot_ftrs]), columns = ohe.get_feature_names())\n",
    "        X_c_ord = pd.DataFrame(data=orde.transform(X_CV[ord_ftrs]), columns = ord_ftrs)\n",
    "\n",
    "        X_t_con = pd.DataFrame(data=scaler.transform(X_test[con_ftrs]), columns = con_ftrs)\n",
    "        X_t_onehot = pd.DataFrame(data=ohe.transform(X_test[onehot_ftrs]), columns = ohe.get_feature_names())\n",
    "        X_t_ord = pd.DataFrame(data=orde.transform(X_test[ord_ftrs]), columns = ord_ftrs)\n",
    "\n",
    "        X_train_df = pd.concat([X_train_con, X_train_onehot, X_train_ord],axis = 1)\n",
    "        X_c_df = pd.concat([X_c_con, X_c_onehot, X_c_ord],axis = 1)\n",
    "        X_t_df = pd.concat([X_t_con, X_t_onehot, X_t_ord],axis = 1)\n",
    "\n",
    "        # tune lasso hyper-parameter, alpha\n",
    "        alpha = np.logspace(-5,3,num=20)\n",
    "        CV_score = []\n",
    "        regs = []\n",
    "        for a in alpha:\n",
    "            reg = Ridge(alpha = a)\n",
    "            reg.fit(X_train_df,y_train)\n",
    "            CV_score.append(reg.score(X_c_df, y_CV))\n",
    "            regs.append(reg)\n",
    "\n",
    "        # find the best alpha in this fold\n",
    "        alpha_opt.append(alpha[np.argmax(CV_score)])\n",
    "        # grab the best model\n",
    "        reg = regs[np.argmax(CV_score)]\n",
    "        # calculate test score using thee best model\n",
    "        test_scores.append(reg.score(X_t_df, y_test))\n",
    "\n",
    "    best_alpha = alpha_opt[np.argmax(test_scores)]\n",
    "    return best_alpha, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.555947811685144,\n",
       " [0.057015784601311965,\n",
       "  0.06349610449222631,\n",
       "  0.05043564963310964,\n",
       "  0.05470348104103173,\n",
       "  0.06324519977035159])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_pipeline_Ridge(X,y,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state: 0 best_alpha: 7.847599703514606\n",
      "random_state: 42 best_alpha: 54.555947811685144\n",
      "random_state: 84 best_alpha: 2.976351441631313\n",
      "random_state: 126 best_alpha: 20.6913808111479\n",
      "random_state: 168 best_alpha: 20.6913808111479\n",
      "random_state: 210 best_alpha: 54.555947811685144\n",
      "random_state: 252 best_alpha: 20.6913808111479\n",
      "random_state: 294 best_alpha: 20.6913808111479\n",
      "random_state: 336 best_alpha: 143.844988828766\n",
      "random_state: 378 best_alpha: 54.555947811685144\n",
      "test score: 0.065 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    best_alpha, test_score = ML_pipeline_Ridge(X,y,i*42)\n",
    "    test_scores.append(test_score)\n",
    "    print('random_state:', i*42, 'best_alpha:', best_alpha)\n",
    "\n",
    "print('test score:',np.around(np.mean(test_scores),3),'+/-',np.around(np.std(test_scores),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def ML_pipeline_RF(X,y,random_state):\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    test_scores = []\n",
    "    bestpara_opt = []\n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "\n",
    "    for train_index, CV_index in kf.split(X_other,y_other):\n",
    "        X_train, X_CV = X_other.iloc[train_index.tolist()], X_other.iloc[CV_index.tolist()]\n",
    "        y_train, y_CV = y_other.iloc[train_index.tolist()], y_other.iloc[CV_index.tolist()]\n",
    "        \n",
    "        con_ftrs = ['Reviews','Size','Price','Updated Days']\n",
    "        onehot_ftrs = ['Category','Size varies','Type','Content Rating','First Genre','Second Genre',\\\n",
    "                       'Current Ver','Current ver varies','Min Ver','Max Ver','Android ver varies']\n",
    "        ord_ftrs = ['Installs']\n",
    "        ord_cats = [['1+','5+','10+','50+','100+','500+','1,000+','5,000+','10,000+','50,000+','100,000+','500,000+',\\\n",
    "                     '1,000,000+','5,000,000+','10,000,000+','50,000,000+','100,000,000+','500,000,000+','1,000,000,000+']]\n",
    "\n",
    "        ohe = OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore')\n",
    "        orde = OrdinalEncoder(categories = ord_cats)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_train_con = pd.DataFrame(data=scaler.fit_transform(X_train[con_ftrs]), columns = con_ftrs)\n",
    "        X_train_onehot = pd.DataFrame(data=ohe.fit_transform(X_train[onehot_ftrs]), columns = ohe.get_feature_names())\n",
    "        X_train_ord = pd.DataFrame(data=orde.fit_transform(X_train[ord_ftrs]), columns = ord_ftrs)\n",
    "\n",
    "        X_c_con = pd.DataFrame(data=scaler.transform(X_CV[con_ftrs]), columns = con_ftrs)\n",
    "        X_c_onehot = pd.DataFrame(data=ohe.transform(X_CV[onehot_ftrs]), columns = ohe.get_feature_names())\n",
    "        X_c_ord = pd.DataFrame(data=orde.transform(X_CV[ord_ftrs]), columns = ord_ftrs)\n",
    "\n",
    "        X_t_con = pd.DataFrame(data=scaler.transform(X_test[con_ftrs]), columns = con_ftrs)\n",
    "        X_t_onehot = pd.DataFrame(data=ohe.transform(X_test[onehot_ftrs]), columns = ohe.get_feature_names())\n",
    "        X_t_ord = pd.DataFrame(data=orde.transform(X_test[ord_ftrs]), columns = ord_ftrs)\n",
    "\n",
    "        X_train_df = pd.concat([X_train_con, X_train_onehot, X_train_ord],axis = 1)\n",
    "        X_c_df = pd.concat([X_c_con, X_c_onehot, X_c_ord],axis = 1)\n",
    "        X_t_df = pd.concat([X_t_con, X_t_onehot, X_t_ord],axis = 1)\n",
    "\n",
    "        # tune hyper-parameter\n",
    "        depth = [d for d in range(1,10)]\n",
    "        features = ['auto','sqrt','log2']\n",
    "        dep_fea = [(d, f) for d in depth for f in features]\n",
    "        CV_score_rf = []\n",
    "        rfs = []\n",
    "        for d, f in dep_fea:\n",
    "            clf = RandomForestRegressor(n_estimators=100,max_depth=d,max_features=f,random_state=random_state)\n",
    "            clf.fit(X_train_df,y_train)\n",
    "            CV_score_rf.append(clf.score(X_c_df,y_CV))\n",
    "            rfs.append(clf)\n",
    "        \n",
    "        best_rf = max(CV_score_rf)\n",
    "        bestpara_opt.append(dep_fea[CV_score_rf.index(best_rf)])\n",
    "        \n",
    "        # grab the best model\n",
    "        rf = rfs[np.argmax(CV_score_rf)]\n",
    "        # calculate test score using the best model\n",
    "        test_scores.append(rf.score(X_t_df, y_test))\n",
    "    best_para = bestpara_opt[np.argmax(test_scores)]\n",
    "    return best_para, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 'auto'),\n",
       " [0.1493370465357815,\n",
       "  0.1353549857102454,\n",
       "  0.14961132970489732,\n",
       "  0.1446092093317335,\n",
       "  0.12777972564636275])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_pipeline_RF(X,y,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
